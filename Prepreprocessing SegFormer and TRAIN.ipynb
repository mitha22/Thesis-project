{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1639,"status":"ok","timestamp":1716560556654,"user":{"displayName":"Mithila Thangaraj","userId":"04228440970312958375"},"user_tz":-120},"id":"lW4lqnQirn2X","outputId":"4f0401a2-5f57-428c-c7b4-12f36e8009a1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"]}],"source":["#Because this was done in GoogleColab, mounting was a neccesity.\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n"]},{"cell_type":"markdown","metadata":{"id":"EupCZhXsp-mK"},"source":["Installation of the different libraries."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"collapsed":true,"executionInfo":{"elapsed":163515,"status":"ok","timestamp":1716560549406,"user":{"displayName":"Mithila Thangaraj","userId":"04228440970312958375"},"user_tz":-120},"id":"WFlVUq3luGHK","outputId":"12ac4026-b253-4557-91a7-ff82ca584146"},"outputs":[],"source":["!pip install datasets\n","!pip install albumentations\n","!pip install torchmetrics\n","!pip install transformers\n","!pip install torch torchvision albumentations\n","!pip install wandb\n","!pip install transformers[torch] accelerate\n","!pip install 'transformers[torch]' -U\n","!pip uninstall accelerate\n","!pip install accelerate\n","!pip install transform\n"]},{"cell_type":"markdown","metadata":{"id":"aRPmAdLFqCM1"},"source":["Importing all the libraries needed to train the model and prepare the images and dataset for it."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ix1j0RNJsuTS"},"outputs":[],"source":["import glob\n","from pathlib import Path\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","from PIL import Image\n","import numpy as np\n","import torch\n","from torch.utils.data import Dataset\n","from torchmetrics import JaccardIn dex, Precision, Recall, F1Score\n","from torch.nn.functional import interpolate\n","import wandb\n","from transformers import TrainingArguments, Trainer, EarlyStoppingCallback, SegformerConfig, SegformerForSemanticSegmentation, TrainerCallback, TrainerControl, TrainerState, Trainer\n","import matplotlib.pyplot as plt\n","from torch.utils.data import DataLoader\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GOg1NxKRvWvq"},"outputs":[],"source":["#function to load images in RGB.\n","def load_image(path):\n","    return Image.open(path).convert('RGB')\n","\n","#function to assign a color channel to a label either 1 or 2. 0 is background and is assigned to black color channel.\n","def create_segmentation_map(mask):\n","    segmap = np.zeros(mask.shape[:2], dtype=np.uint8)  #Background\n","    segmap[mask[:, :, 1] == 255] = 1  #LOW_1_3\n","    segmap[mask[:, :, 2] == 255] = 2  #HIGH_4_5\n","    return segmap\n","\n","\n","#the class CustomDataset is from Pytorch library and is used to create the dataset for training the model.\n","#sourcecode: https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n","#CustomDataset composes a dataset of images and masks.\n","class CustomDataset(Dataset):\n","    def __init__(self, image_paths, label_paths):\n","        self.image_paths = image_paths\n","        self.label_paths = label_paths\n","        #Data Augmentation is defined for the images here. Albumentations library is used for this. As described in the report, resizing, horizontal-, and verticalflip, randombrightness and normalization is defined.\n","        self.transform = A.Compose([\n","            A.Resize(256, 256),\n","            A.HorizontalFlip(p=0.5),\n","            A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n","            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n","            A.VerticalFlip(p=0.5),\n","            #converting the image into tensor format because the model only takes images converted to tensors as input.\n","            ToTensorV2(),\n","        ])\n","        #Data Augmentation is defined for the masks as well.\n","        self.mask_transform = A.Compose([\n","            A.Resize(256, 256),\n","            A.HorizontalFlip(p=0.5),\n","            A.VerticalFlip(p=0.5),\n","            #converting the mask into tensor format because the model only takes masks converted to tensors as input.\n","            ToTensorV2(),\n","        ])\n","#returns number of samples in dataset\n","    def __len__(self):\n","        return len(self.image_paths)\n","\n","#Loads and return a single sample (image and the corresponding mask) from the dataset at the specified index, 'idx'. create_segmentation_map is also applied here such that each color channel is applied to a label.\n","    def __getitem__(self, idx):\n","        image = np.array(load_image(self.image_paths[idx]))\n","        mask = np.array(load_image(self.label_paths[idx]))\n","        mask = create_segmentation_map(mask)\n","\n","        #augmentation is applied image\n","        transformed_image = self.transform(image=image)\n","        image = transformed_image['image']\n","\n","        #augmentation is applied to mask\n","        transformed_mask = self.mask_transform(image=mask)\n","        mask = transformed_mask['image']\n","\n","        #Kept getting a bug of an extra channel dimension, so this will be removed by using squeeze.\n","        mask = mask.squeeze(0)  # Remove channel dimension if present\n","\n","        return {\"pixel_values\": image, \"labels\": mask.long()}\n","\n","\n","#this class is used during training to load the batch samples to the model.\n","#sourcecode: https://huggingface.co/docs/transformers/main_classes/data_collator\n","class CustomDataCollator:\n","    def __call__(self, batch):\n","      #transformed images (tensors) is used from the function right above 'getitem'. These images are tensors are stacked using the torch.stack function from Pytorch library. Please note it will be loaded as batches to the model.\n","        pixel_values = torch.stack([item['pixel_values'] for item in batch])\n","        #transformed masks (tensors) is used from the function right above 'getitem'. These masks are tensors are stacked using the torch.stack function from Pytorch library. Please note it will be loaded as batches to the model.\n","        labels = torch.stack([item['labels'] for item in batch])\n","        return {'pixel_values': pixel_values, 'labels': labels}\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1OKPsJ6zJkAV"},"outputs":[],"source":["#sourcecode: https://lightning.ai/docs/torchmetrics/stable/classification/f1_score.html, https://lightning.ai/docs/torchmetrics/stable/classification/jaccard_index.html, https://lightning.ai/docs/torchmetrics/stable/classification/precision.html, https://lightning.ai/docs/torchmetrics/stable/classification/recall.html\n","#The metrics are calculated using torch metrics.\n","#For each metric, number of classes is defined, task is multiclass because there are 3 number of labels, background is ignored during training.\n","jaccard_index = JaccardIndex(num_classes=3, task=\"multiclass\", ignore_index=0)\n","precision_metric = Precision(num_classes=3,task=\"multiclass\", ignore_index=0)\n","recall_metric = Recall(num_classes=3,task=\"multiclass\", ignore_index=0)\n","f1_metric = F1Score(num_classes=3, task=\"multiclass\", ignore_index=0)\n","\n","#sourcecode: https://www.kaggle.com/code/italyforever/drone-images-segmentation\n","#Compute metrics is used to calculate the evaluation metrics. The evaluation is done by using logits and labels.\n","def compute_metrics(eval_pred):\n","    logits, labels = eval_pred\n","\n","    #'isinstance' is used to check if the logits and labels are in tensor form or not.\n","    logits = torch.tensor(logits) if not isinstance(logits, torch.Tensor) else logits\n","    labels = torch.tensor(labels, dtype=torch.long) if not isinstance(labels, torch.Tensor) else labels\n","    #argmax returns the probability of the image pixels belongs to the classes. a tensor is returned,  where each value represents the most likely class for the corresponding pixel.\n","    preds = torch.argmax(logits, dim=1)\n","    #Because the torch.argmax returns a tensor with dimensions (batch_size, num_classes, height, width), a dimension is added by unsqeezing the tensor; (batch_size, 1, height, width) which is neccesary in order to be able to\n","    #interpolate the tensor. Interpolation of the tensor is done such that it matches the label tensors. Using nearestneighbor the interpolation/reszing is done.\n","    preds = torch.nn.functional.interpolate(preds.unsqueeze(1).float(), size=labels.shape[-2:], mode='nearest').squeeze(1)\n","\n","    #resetting all the metrics to ensure that every run starts from scratch.\n","    jaccard_index.reset()\n","    precision_metric.reset()\n","    recall_metric.reset()\n","    f1_metric.reset()\n","\n","    #metrics gets updated during evaluation\n","    jaccard_index.update(preds, labels)\n","    precision_metric.update(preds, labels)\n","    recall_metric.update(preds, labels)\n","    f1_metric.update(preds, labels)\n","\n","    #final metrics computed for evaluation.\n","    ious = jaccard_index.compute()\n","    precision = precision_metric.compute()\n","    recall = recall_metric.compute()\n","    f1 = f1_metric.compute()\n","\n","    return {\n","        \"jaccard_index/overall\": ious.mean().item(),\n","        \"jaccard_index/class_0\": ious[0].item(),\n","        \"jaccard_index/class_1\": ious[1].item(),\n","        \"jaccard_index/class_2\": ious[2].item(),\n","        \"precision/overall\": precision.mean().item(),\n","        \"precision/class_0\": precision[0].item(),\n","        \"precision/class_1\": precision[1].item(),\n","        \"precision/class_2\": precision[2].item(),\n","        \"recall/overall\": recall.mean().item(),\n","        \"recall/class_0\": recall[0].item(),\n","        \"recall/class_1\": recall[1].item(),\n","        \"recall/class_2\": recall[2].item(),\n","        \"f1/overall\": f1.mean().item(),\n","        \"f1/class_0\": f1[0].item(),\n","        \"f1/class_1\": f1[1].item(),\n","        \"f1/class_2\": f1[2].item()\n","    }\n"]},{"cell_type":"markdown","metadata":{"id":"O9c6BKQGp2AO"},"source":["The Trainer callback doesn't include metrics such as IOU for training, the callback can be extended using the class TrainingMetricsLoggingCallback. This class comes from the huggingface library transformers and allows customization of logged metrics during training.\n","\n","\n","sourcecode: https://huggingface.co/transformers/v4.6.0/_modules/transformers/trainer_callback.html#TrainerCallback.on_epoch_end, https://huggingface.co/docs/transformers/main_classes/callback"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"erlAeNCpgjD6"},"outputs":[],"source":["#TrainingMetricsLoggingCallback is used to customize the Trainer callback used to train and evaluate the model.\n","class TrainingMetricsLoggingCallback(TrainerCallback):\n","  #During training the definition or the customization is called at the end of every epoch\n","    def on_epoch_end(self, args, state: TrainerState, control: TrainerControl, model, **kwargs):\n","      #making sure the whole train dataset is used\n","        global train_dataset\n","\n","        #Initializing metrics that should be included in the trainer callback\n","        jaccard_index = JaccardIndex(num_classes=3, task=\"multiclass\", ignore_index=0)\n","        jaccard_index.reset()\n","        #DataLoader is from Pytorch library and is used to load the train_data.\n","        data_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args.per_device_eval_batch_size, collate_fn=CustomDataCollator())\n","        #iterating over the data_loader in batches\n","        for batch in data_loader:\n","          #batch is a dictionary consistent of keys, k and v (this will be pixel_values and labels),\n","            batch = {k:v for k, v in batch.items()}\n","            #the forward pass is wrapped because the weights does not need to be updated during inference. Torch.no_grad is used to do this.\n","            with torch.no_grad():\n","              #performing a forward pass of the model with the batch of input data. The **batch syntax unpacks the dictionary 'batch' (from line 13) and passes its items as keyword arguments to the model.\n","                outputs = model(**batch)\n","                #extracting raw prediction scores (logits) from the model output\n","            logits = outputs.logits\n","            #finding the most likely class for each pixel by taking the argmax along the class dimension\n","            preds = logits.argmax(dim=1)\n","\n","            #resizing predictions using interpolate just like in the code chunk above.\n","            preds = preds.unsqueeze(1)  #adding a channel dimension\n","            preds = interpolate(preds.float(), size=batch['labels'].shape[-2:], mode='nearest') #using nearest neighbor method to interpolate the labels.\n","            preds = preds.squeeze(1)  #remocing the channel dimension\n","            #jaccard_index is used again from the Torchmetrics; https://lightning.ai/docs/torchmetrics/stable/classification/jaccard_index.html\n","            jaccard_index.update(preds, batch['labels']) #updating IOU metric for the prediction and labels in the batch.\n","\n","        train_iou = jaccard_index.compute().mean().item() #IOU is computed overall and is returned as a tensor first but is then converted when using .item() as a pythonscalar such that the metric can be logged\n","        wandb.log({'train_iou': train_iou}, step=state.epoch) #train_iou is logged to wandb; https://docs.wandb.ai/guides/integrations/huggingface\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"IIlJ_X_H9paZ"},"source":["All functions and callbacks prior to training and inference is now done. Now the dataset need to be loaded and defined."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ASNjDmeEvg-c"},"outputs":[],"source":["#Using pathlib library to load the datasets from the respective image and mask directories.\n","image_dir = Path('/content/gdrive/MyDrive/speciale/export1/assets')\n","label_dir = Path('/content/gdrive/MyDrive/speciale/export1/labels')\n","#Glob library is used to sort the images and masks such that when the directories are used, the image and mask is aligned in pairs.\n","image_paths = sorted(glob.glob(str(image_dir / '*.jpg')))\n","label_paths = sorted(glob.glob(str(label_dir / '*.png')))\n","\n","#class CustomDataset is used here to create the dataset (note that the directories from line 2 and 3 are used here)\n","dataset = CustomDataset(image_paths, label_paths)\n","#the size of the dataset is defined as 80% of the overall dataset size.\n","train_size = int(0.8 * len(dataset))\n","\n","# the rest will be for the test dataset, which is 20%.\n","train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, len(dataset) - train_size])\n"]},{"cell_type":"markdown","metadata":{"id":"MDryaUcFARvx"},"source":["id2label and label2id is used to map the labels to the corresponding colorrange. Please note that id2label is purely for the sake of being able to visualize the predictions of the masks.\n","\n","sourcecode: https://github.com/huggingface/notebooks/blob/main/examples/semantic_segmentation.ipynb"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":238,"status":"ok","timestamp":1716560586395,"user":{"displayName":"Mithila Thangaraj","userId":"04228440970312958375"},"user_tz":-120},"id":"eh5XS6L2wMAd","outputId":"ab447818-e30c-4396-ba6f-1bc47e10f8fe"},"outputs":[],"source":["#assigning the labels of each label.\n","id2label = {\n","    0: 'BACKGROUND',\n","    1: 'LOW_1_3',\n","    2: 'HIGH_4_5'\n","}\n","\n","#defining label to id mapping by inverting id2label\n","label2id = {label: id for id, label in id2label.items()}\n","\n"]},{"cell_type":"markdown","metadata":{"id":"KyBQJYe1EAAo"},"source":["All results are logged to wandb which is a platform that can generate plots for the specified metrics. source: https://docs.wandb.ai/guides/integrations/huggingface"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":115,"referenced_widgets":["0bcaccfd06d54298bf8d42fa62ab655e"]},"id":"wyk91OYO10Wv","outputId":"03c6a9c2-1be8-4d2c-bda6-c4299d843d74"},"outputs":[],"source":["wandb.login()\n","wandb.init(project=\"Pytorch\", entity=\"mitth\") #login to wandb and the dashboard shows up."]},{"cell_type":"markdown","metadata":{"id":"7HL4pXWfEVqV"},"source":["Using the pretrained models from huggingfaces modelhub:\n","SegFormer B0:https://huggingface.co/nvidia/segformer-b0-finetuned-cityscapes-768-768\n","SegFormer B3: https://huggingface.co/nvidia/segformer-b3-finetuned-cityscapes-1024-1024\n","SegFormer B5: https://huggingface.co/nvidia/segformer-b5-finetuned-cityscapes-1024-1024"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D2kJ8YVZwRv8"},"outputs":[],"source":["#using the following github repository as pipeline for training the model: https://github.com/huggingface/notebooks/blob/main/examples/semantic_segmentation.ipynb\n","#definting the model name from the huggingface model hub.\n","pretrained_model_name = \"nvidia/segformer-b5-finetuned-cityscapes-1024-1024\"\n","#loading the model configurations for SegFormer and the number of labels is set as 3 as there are 3 labels.\n","config = SegformerConfig.from_pretrained(pretrained_model_name, num_labels=3)\n","#loading the pretrained model with the corresponding defined configurations.\n","model = SegformerForSemanticSegmentation.from_pretrained(pretrained_model_name, config=config)\n","\n","\n","#TrainingArguments is an instance of transformers library and is used to define the training arguments.\n","#sourcecode: https://huggingface.co/docs/transformers/v4.41.2/en/main_classes/trainer#transformers.TrainingArguments\n","training_args = TrainingArguments(\n","    #specification for saving strategy for evaluation; should be saved at the end of each epoch\n","    evaluation_strategy=\"epoch\",\n","    #directory for logging metrics.\n","    logging_dir=\"logs\",\n","    #All logging are done by the end of each epoch\n","    logging_strategy=\"epoch\",\n","    #batch size for training set\n","    per_device_train_batch_size=23,\n","    #batch size for inference/test set\n","    per_device_eval_batch_size=23,\n","    #setting the number of epochs for training\n","    num_train_epochs=150,\n","    #setting (maximum due to linear scheduling) learning rate\n","    learning_rate=0.0005,\n","    #L2-regularization constant\n","    weight_decay=0.01,\n","    #ensuring the results are logged to wandb.\n","    report_to=\"wandb\",\n","    run_name=\"segformer-training-run\",\n","    #proportion of warmup phase; set because earlystopping kicked in too early and it was impossible to tell if the model was learning or not.\n","    warmup_ratio=0.1,\n","    #saving stategy for saving the best model; will happen at the end of each epoch\n","    save_strategy=\"epoch\",\n","    #best model will be saved.\n","    load_best_model_at_end=True\n",")\n","\n","\n","#Here the model will start training using Trainer callback from transformers library. The model, trainin_args, train_dataset, test_dataset,\n","#metrics to be logged during evaluation (compute_metrics) and training (Trainer () and TraningMetricsLoggingCallback() are all predefined and is simply just called in this section. )\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=test_dataset,\n","    compute_metrics=compute_metrics,\n","    callbacks=[TrainingMetricsLoggingCallback()]\n",")\n","\n","#Earlystopping callback is used from transformers library. Sourcecode: https://towardsdatascience.com/fine-tuning-pretrained-nlp-models-with-huggingfaces-trainer-6326a4456e7b\n","early_stopping_callback = EarlyStoppingCallback(\n","    early_stopping_patience=15,\n","    early_stopping_threshold=0.001\n",")\n","\n","\n","\n","#Starting the training process.\n","trainer.train()"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyMgq0uKbN3au7waYwAyUKvV","gpuType":"L4","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
